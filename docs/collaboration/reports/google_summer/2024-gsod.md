
# BIDS - INCF 2024 Google Season of Docs Case Study


## Umbrella Organization [INCF](incf.org) International Neuroinformatics  Coordinating Facility


## Project Organization	[BIDS](https://bids.neuroimaging.io/) Brain Imaging Data Structure


## Project Title 	[Streamlining the BIDS Online Presence](https://www.incf.org/activities/gsod)

#### About BIDS  


The Brain Imaging Data Structure (BIDS) is an open global community driving the standardization of neuroscience data across a broad and growing range of modalities and health research disciplines. First released in June 2016, it is supported by a worldwide research network and endorsed by organizations like the International Neuroinformatics Coordinating Facility (INCF). A very large community, including ~29K yearly visitors, use the BIDS specification. More than 300 BIDS contributors currently support and maintain the BIDS community resources to structure and share their data. BIDS encompasses over 40 domain-specific and modality-specific technical specifications, open software conversion and analytics tools, and global infrastructure for collaborating on emerging standards in neuroscience ([Poldrack 2024](https://doi.org/10.1162/imag_a_00103)).

Case Study Authors		Christine Rogers &lt;[christine.rogers@mcgill.ca](mailto:christine.rogers@mcgill.ca)>


Eric Earl
on behalf of the project participants


# Summary

*Fill out the following table and provide brief answers to the questions. You can provide more details in later sections.** We recommend you write out the detailed sections first then come back to write the summary.***


<table>
  <tr>
   <td><strong># Tech Writers</strong>
   </td>
   <td><strong>TW Project Hours</strong>
   </td>
   <td><strong>Budget</strong>
   </td>
   <td><strong>% Project Completed</strong>
   </td>
  </tr>
  <tr>
   <td>1
   </td>
   <td>150
   </td>
   <td>$6,500 USD
   </td>
   <td>100%
   </td>
  </tr>
</table>




* What problem were you trying to solve? And how did you try to solve it?
* What were the outcomes of your project?
* What went well? And what would you do differently?
* What advice would you give to other projects trying to solve a similar problem with documentation?


# Project Description


## Project Proposal

*Provide a brief summary of your proposal and a link to the proposal page on your project site.*

[https://www.incf.org/activities/gsod](https://www.incf.org/activities/gsod)

The Brain Imaging Data Structure (BIDS) is a global initiative to standardize neuroscience data across various modalities. Despite its widespread use and contributions from over 300 community members, the prior online resources distributed across multiple platforms presented barriers to new user onboarding and effective contribution. This project aimed to streamline and centralize BIDS documentation and resources, making them more accessible, user-friendly, and aligned with FAIR (Findable, Accessible, Interoperable, Reusable) guiding principles.

While the BIDS main website and BIDS technical specification already see high and growing usage, this project lowered barriers in onboarding new users and facilitating new user contributions to the community by improving the BIDS main website. Improvements from this project satisfied FAIR guiding principles and improved the experience of referencing BIDS documentation for new and old users alike.


## Proposal Creation Process

*How did you come up with your Google Season of Docs proposal? What process did your organization use to decide on an idea? How did you solicit and incorporate feedback? Who participated in the process? How long did it take?*

The BIDS maintainers, steering group, and BIDS online presence working group all saw a need for BIDS to improve and centralize its online resources. One of the BIDS maintainers, Christine Rogers, had worked with Google Season of Docs before with success and suggested we partner with INCF to make progress on the website and documentation improvement projects. Primarily Christine and Eric Earl (chair of the BIDS online presence working group and another BIDS maintainer) drove the project proposal forward with prioritization oversight from the online presence working group. The brief proposal format was effectively developed through several consultative meetings and rounds of drafting.


## Budget


<table>
  <tr>
   <td>How much money did you ask for?
   </td>
   <td>$6,500 USD
   </td>
  </tr>
  <tr>
   <td>How did you come up with this estimate?
   </td>
   <td>Writer stipend:  $5000  \
Mentor stipend:  $500 * 3 mentors
<p>
(following GSOC practice)
   </td>
  </tr>
  <tr>
   <td>How many hours of work did you budget for the project?
   </td>
   <td>Writer: 4 h / wk \
Mentors; 1 h / wk * 4 mentors = 4 h / wk
   </td>
  </tr>
  <tr>
   <td>How many hours of work were actually needed for the project?
   </td>
   <td>Writer: 5 h / wk  \
Mentors: 1.5 h / week * 4 mentors = 6 h / wk
   </td>
  </tr>
  <tr>
   <td>What other expenses did you include in your budget?
   </td>
   <td>None
   </td>
  </tr>
  <tr>
   <td>Did you run into any budget surprises during the project (e.g. misestimates)? If so, please explain.
   </td>
   <td>Mentor time underestimated. Would allocate  more for additional mentors or more mentor time.
   </td>
  </tr>
</table>



## Tech Writer Recruitment

*How did you find and hire your technical writer? Did the process work well? Did they stay for the duration of the project?*

We selected a technical writer candidate early on based on the advanced neuroinformatics data standards experience which was fundamental to quickly on-board for this project.

When INCF published the writer and mentor name on the [project page](https://www.incf.org/activities/gsod), first the writer and then mentors were personally contacted by individuals wishing to participate.

Note: Writer name, but not his contact information was published. For that reason we took his name down and wrote that a writer had already been selected.

We were lucky to work with this writer with significant experience and technical background in the field for the full duration of the project timeline from start to end.


## Other Participants

*Who else worked on this project (use usernames if requested by participants)? How did you recruit them? What roles did they have? Did they stay for the duration of the project?*

A few BIDS maintainers were recruited to mentor the project. Each stayed for the duration of the project with varying availability over the year.

Additional members of the Maintainers and Steering group were recruited to provide feedback on the work in progress. In addition, a feedback survey was distributed globally to the user community with feedback received at the 60% progress mark in the project.


## Timeline

*Give a short overview of the timeline of your project (indicate estimated end date or intermediate milestones if project is ongoing). Did the original timeline need adjustment?*

April and May were a soft start to the project, with our tech writer and mentors alike getting used to working out project tracking and bi-weekly check-in meetings with each other. A lot of technical changes happened early on to combine many documentation sites into just one website and improve navigation. By June and July we were requesting and incorporating feedback from contributor surveys. We were able to share progress and contributor views during the annual Organization for Human Brain Mapping (OHBM) Conference’s BIDS Town Hall. August and September was another round of requesting and incorporating feedback from web surveys. And finally, October and November brought the BIDS Impact page together to wrap the project with a significant effort on everyone’s part.


## Deliverables

*Fill in the following tables to describe what was created, updated, or otherwise changed during the project. Include both planned and unplanned deliverables. Be sure to include things that were planned but not completed.*


#### Planned Deliverables


<table>
  <tr>
   <td><strong>Deliverable</strong>
   </td>
   <td><strong>% Complete</strong>
   </td>
   <td><strong>Relevant Links</strong>
   </td>
   <td><strong>Notes</strong>
   </td>
  </tr>
  <tr>
   <td>Consolidate into one website
   </td>
   <td>100%
   </td>
   <td><a href="https://bids-website.readthedocs.io/">https://bids-website.readthedocs.io/</a>
   </td>
   <td>Planning launch in Q1 2025 to replace the old BIDS website at <a href="https://bids.neuroimaging.io/">https://bids.neuroimaging.io/</a>
   </td>
  </tr>
  <tr>
   <td>Implement a more friendly main website structure and improve navigation
   </td>
   <td>100%
   </td>
   <td><a href="https://github.com/bids-standard/bids-website/blob/main/mkdocs.yml#L88">https://github.com/bids-standard/bids-website/blob/main/mkdocs.yml#L88</a>
   </td>
   <td>User feedback indicates this created significant improvement
   </td>
  </tr>
  <tr>
   <td>New BIDS Impact page
   </td>
   <td>100%
   </td>
   <td><a href="https://bids-website.readthedocs.io/en/latest/impact/index.html">https://bids-website.readthedocs.io/en/latest/impact/index.html</a>
   </td>
   <td>Expanded and improved with more metrics and relevant context
   </td>
  </tr>
  <tr>
   <td>Categorize and add clear summaries to each webpage
   </td>
   <td>100%
   </td>
   <td><a href="https://github.com/bids-standard/bids-website/pull/498">https://github.com/bids-standard/bids-website/pull/498</a>
   </td>
   <td>Each page now has a lead paragraph
   </td>
  </tr>
  <tr>
   <td>Synthesized and enabled gathering of user-feedback and user-testing
   </td>
   <td>100%
   </td>
   <td>
   </td>
   <td>Developed a follow-up survey and collated findings from all community feedback. Helped synthesize metrics and inputs for annual community presentation.
   </td>
  </tr>
  <tr>
   <td>Add a feedback form within the main website
   </td>
   <td>100%
   </td>
   <td>
   </td>
   <td>Giscus at the bottom of the front page was re-scoped and implemented by a mentor
   </td>
  </tr>
</table>


The above table summarizes the many changes across several repositories (PRs linked here):



* [BIDS Website repo](https://github.com/bids-standard/bids-website/pulls?q=+is%3Apr+author%3Aiamdamion) and sub/related repos with content:
* [BIDS-Apps Repo](https://github.com/bids-apps/bids-apps.github.io/pull/140)
* [BIDS Execution Spec](https://github.com/bids-standard/execution-spec/pull/24)
* [BIDS Extensions repo](https://github.com/bids-standard/bids-extensions/pull/35)


#### Unplanned Deliverables

*Were there any deliverables created that were not planned at the start of the project? If so, please explain.*

No unplanned deliverables arose in the course of this project.


## Metrics

*What metrics did you choose to measure the success of the project? Were you able to collect those metrics? Did you observe any change as a result of the project? Did you add or remove any metrics since your proposal? How often do you intend to collect metrics going forward?*

***Survey data — What can we glean from the October survey vs. the April survey? ***

We performed a broad-reaching public BIDS community survey (120 respondents) before this project started and a narrower website-specific survey at the 60% mark (19 respondents) in this project’s timeline. The results were overwhelmingly positive. For instance, when asked to rate the overall utility of the old vs the new website, it is was overwhelmingly rated as “OK” (nearly two-thirds of respondents on the scale: Bad, OK, Good, or Great). Some of the improvements mentioned were: acceleration in accessing info, Y was appreciated and Z demonstrated that this project helped significantly in clarifying understanding of the material. * *


# Analysis

*Provide a short narrative about how the project went. Was the project successful? Why or why not? Or when will you be able to judge success? Did you face any unexpected hurdles or setbacks? Did the project result in any new or updated processes or procedures in your organization?*

This was a successful project. The project resulted in positive user feedback about the changes implemented by only 60% through the timeline.

During on-boarding as we began breaking down deliverables into tasks, the mentors realized that a few deliverables could be resolved most effectively by mentors managing the most technical steps. This minor re-scoping helped move the project forward quickly. Some timelines were delayed due to outside factors during the course of the project and timelines were adjusted accordingly.

 \
The project experience underscored the value of external support for open-source initiatives – both in terms of funding and umbrella organization administration.


## Lessons Learned

*Use the following tables to highlight lessons learned -- for things that went well and things that could be improved. Use the listed topic categories as appropriate or add your own. Feel free to have multiple rows for a given topic as needed. These lessons learned will feed into [our aggregated list](https://github.com/google/season-of-docs/tree/main/resources/best-practices.md) of best practices for documentation projects.*


#### What went well?

*For lessons learned, add your own or indicate a "plus one" for any of the [existing Google Season of Docs Best Practices](https://github.com/google/season-of-docs/tree/main/resources/best-practices.md).*


<table>
  <tr>
   <td><strong>Topic</strong>
   </td>
   <td><strong>What we did</strong>
   </td>
   <td><strong>Lesson learned</strong>
   </td>
  </tr>
  <tr>
   <td>Mentorship
   </td>
   <td>Had a pool of 5 potential mentors for support
   </td>
   <td>Planning for a larger pool of mentors was an excellent idea.
   </td>
  </tr>
  <tr>
   <td>Metrics
   </td>
   <td>Sent a “before” and “after” community survey
   </td>
   <td>Do this again, and sooner/better timed to the changes
   </td>
  </tr>
  <tr>
   <td>Project Deliverables
   </td>
   <td>Resolved some deliverables with technical features on the website (e.g. feedback form, search) not technical writing
   </td>
   <td>Re-scoping project can be okay to solve challenges in the most effective way
   </td>
  </tr>
</table>



#### What could be improved?

*For lessons learned, add your own or indicate a "plus one" for any of the existing Google Season of Docs Best Practices.*


<table>
  <tr>
   <td><strong>Topic</strong>
   </td>
   <td><strong>What we did</strong>
   </td>
   <td><strong>Lesson learned</strong>
   </td>
  </tr>
  <tr>
   <td>Budget
   </td>
   <td>1 writer stipend; 3 mentor stipends split across 4.
   </td>
   <td>We underestimated mentor effort and would budget more mentor stipends
   </td>
  </tr>
  <tr>
   <td>Communication
   </td>
   <td>Mattermost plus alternating weekly check-in/coworking times
   </td>
   <td>Allocate extra mentor time to managing and setting timelines
   </td>
  </tr>
  <tr>
   <td>Onboarding
   </td>
   <td>Set a loose order of on-boarding tasks
   </td>
   <td>Encourage an order of completion for early tasks e.g. friction log
   </td>
  </tr>
  <tr>
   <td>Project Management
   </td>
   <td>GitHub Project Board
   </td>
   <td>It took a few iterations to get this right. Deliverable timelines between meetings should be set.
   </td>
  </tr>
</table>

```delete 
***Joke but add somewhere: 90% of this project was resolved by adding a search feature***


## Technical Writer Work Record

**List of all Damion’s PRs : **



* BIDS-Website repo: ~1,000 lines of code \
[https://github.com/bids-standard/bids-website/pulls?q=+is%3Apr+author%3Aiamdamion](https://github.com/bids-standard/bids-website/pulls?q=+is%3Apr+author%3Aiamdamion)
* Other repos – Only 3 PRs = about 100 lines
    * Bids-app repo: 1 pr [https://github.com/bids-apps/bids-apps.github.io/pull/140](https://github.com/bids-apps/bids-apps.github.io/pull/140)
    * Execution spec: 1 pr [https://github.com/bids-standard/execution-spec/pull/24](https://github.com/bids-standard/execution-spec/pull/24)
    * BIDS-extensions repo: 1 pr [https://github.com/bids-standard/bids-extensions/pull/35](https://github.com/bids-standard/bids-extensions/pull/35)

**Non-GitHub work** for onboarding / input gathering / community-facing writing [**<span style="text-decoration:underline;">add links to each of the below</span>**]



* New website survey — Cryptpad : feedback comparing old / new website

    Making new questions to and updating old questions from prior survey,


    Integrating mentor input on the survey questions


    Was sent out Oct. 24?  (intention was mid-summer pre/post OHBM)

* Compiled data from spring survey
* Compiled slides for bids townhall
* Documented a lot of what he was doing  – Links

**Scope changes: **



* Remi managed the website and migrated / updated a lot of material.
* Delays with the website materials meant that we [**Christine to fill in this blank**]

**Interesting experiences of note: **

We selected a writer candidate and mentors early on based on the advanced neuroinformatics data standards experience which was fundamental to quickly on-boarding for this project, and published their names on the INCF project page.

That resulted in a few of the project team being persistently contacted by others who wished to be the writer on the project.  (Note his contact information was not published, just his name.)

For that reason we took his name down and wrote that a writer had already been selected.  \
(add link, something better than this page [https://www.incf.org/blog/incf-and-bids-collaborate-google-season-docs-2024](https://www.incf.org/blog/incf-and-bids-collaborate-google-season-docs-2024), maybe this one: [https://www.incf.org/activities/gsod](https://www.incf.org/activities/gsod)) \
 \
**We realized after the program start date that some tasks were better handled by a technical mentor (e.g. the Feedback form became a plug-in installed to interface with GitHub).**
